---
title: "Limitations"
---

This section covers design limitations of PGD, that should be taken into account
when planning your deployment.

## Limits

- PGD can run hundreds of nodes on good-enough hardware and network. However,
for mesh-based deployments, we generally don't recommend running more than
32 nodes in one cluster.
Each master node can be protected by multiple physical or logical standby nodes.
There's no specific limit on the number of standby nodes,
but typical usage is to have 2&ndash;3 standbys per master. Standby nodes don't
add connections to the mesh network, so they aren't included in the
32-node recommendation.

- PGD currently has a hard limit of no more than 1000 active nodes, as this is the
current maximum Raft connections allowed.

- Support for using EDB Postgres Distributed for multiple databases on the same
Postgres instance is deprecated beginning with EDB Postgres Distributed 5 and
will no longer be supported with EDB Postgres Distributed 6. As we extend the
capabilities of the product, the additional complexity introduced operationally
and functionally is no longer viable in a multi-database design.

- The minimum recommended number of nodes in a group is three to provide fault
tolerance for PGD's consensus mechanism. With just two nodes, consensus would
fail if one of the nodes was unresponsive. Consensus is required for some PGD
operations such as distributed sequence generation. For more information about
the consensus mechanism used by EDB Postgres Distributed, see
[Architectural details](../architectures/#architecture-details).


## Other Limitations

This is a (non-comprehensive) list of other limitations that are
expected and are by design. They are not expected to be resolved in the
future and should be taken under consideration when planning your deployment.

-   Replacing a node with its physical standby doesn't work for nodes that
    use CAMO/Eager/Group Commit. Combining physical standbys and EDB Postgres
    Distributed isn't recommended, even if possible.

-   A `galloc` sequence might skip some chunks if the
    sequence is created in a rolled back transaction and then created
    again with the same name. This can also occur if it is created and dropped when DDL
    replication isn't active and then it is created again when DDL
    replication is active.
    The impact of the problem is mild, because the sequence
    guarantees aren't violated. The sequence skips only some
    initial chunks. Also, as a workaround you can specify the
    starting value for the sequence as an argument to the
    `bdr.alter_sequence_set_kind()` function.

-   Legacy synchronous replication uses a mechanism for transaction
    confirmation different from the one used by CAMO, Eager, and Group Commit.
    The two are not compatible and must not be used together. Using synchronous
    replication to other non-PGD nodes, including both logical and physical
    standby is possible.
