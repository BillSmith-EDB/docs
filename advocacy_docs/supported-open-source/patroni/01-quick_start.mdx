---
title: 'Quick Start'
description: "A walkthrough: setup a demo cluster using Patroni and etcd"
---

### Description

This section walks you through the various steps to create an highly available **demo** cluster (using **PostgreSQL** version 14 on Rocky Linux 8) where the etcd host will be called **etcd1** and the 2 PostgreSQL nodes in _Streaming Replication_: **pg-patroni1** and **pg-patroni2**.

![Demo diagram](images/etcd-demo.png)

### Etcd

Etcd is a strongly consistent, distributed key-value store that provides a reliable way to store data that needs to be accessed by a distributed system or cluster of machines.

!!! Note
    For production environments, it is recommended to setup a three nodes cluster. One etcd cluster can serve multiple PostgreSQL/Patroni clusters.

#### Installation

On host **etcd1**, install the latest release:

```bash
$ ETCD_RELEASE=$(curl -s https://api.github.com/repos/etcd-io/etcd/releases/latest|grep tag_name | cut -d '"' -f 4)
$ echo $ETCD_RELEASE
v3.5.4
$ curl -sL https://github.com/etcd-io/etcd/releases/download/${ETCD_RELEASE}/etcd-${ETCD_RELEASE}-linux-amd64.tar.gz \
 | sudo tar xz -C /usr/bin --strip=1 --wildcards --no-anchored etcdctl etcd
$ etcd --version
etcd Version: 3.5.4
```

Create the data directory and system user:

```bash
$ sudo mkdir -p /var/lib/etcd/
$ sudo mkdir /etc/etcd
$ sudo groupadd --system etcd
$ sudo useradd -s /sbin/nologin --system -g etcd etcd
$ sudo chown -R etcd:etcd /var/lib/etcd/
$ sudo chmod 0775 /var/lib/etcd/
```

Configure systemd and start etcd service:

```bash
$ cat <<EOF | sudo tee /etc/systemd/system/etcd.service
[Unit]
Description=etcd key-value store
Documentation=https://github.com/etcd-io/etcd
After=network-online.target local-fs.target remote-fs.target time-sync.target
Wants=network-online.target local-fs.target remote-fs.target time-sync.target

[Service]
User=etcd
Type=notify
Environment=ETCD_DATA_DIR=/var/lib/etcd
Environment=ETCD_NAME=%m
EnvironmentFile=-/etc/etcd/etcd.conf
ExecStart=/usr/bin/etcd --enable-v2=true
Restart=always
RestartSec=10s
LimitNOFILE=40000

[Install]
WantedBy=multi-user.target
EOF

$ sudo systemctl daemon-reload
$ sudo systemctl enable etcd.service
$ sudo systemctl start etcd.service
```

!!! Note
  When starting the etcd service, unless `--enable-v2=true` is specified, etcd v3.4 server would not serve v2 API requests.
  Currently and unfortunately, Patroni doesn't support

Configure and create a new etcd cluster:

```bash
$ MY_IP=$(hostname -I | awk ' {print $1}')
$ cat <<EOF | sudo tee /etc/etcd/etcd.conf
#[Member]
ETCD_LISTEN_PEER_URLS="http://$MY_IP:2380"
ETCD_LISTEN_CLIENT_URLS="http://localhost:2379,http://$MY_IP:2379"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://$MY_IP:2380"
#[Clustering]
ETCD_INITIAL_CLUSTER="etcd0=http://$MY_IP:2380,"
ETCD_ADVERTISE_CLIENT_URLS="http://$MY_IP:2379"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster-1"
ETCD_INITIAL_CLUSTER_STATE="new"
EOF
$ sudo systemctl restart etcd.service
```

The service should be started and listening on ports `2379` and `2380`. Check the cluster status:

```bash
$ ss -tunelp | grep 2379
tcp   LISTEN 0      128          127.0.0.1:2379      0.0.0.0:*    uid:994 ino:27020 sk:d <->
tcp   LISTEN 0      128    192.168.121.180:2379      0.0.0.0:*    uid:994 ino:27017 sk:e <->

$ ss -tunelp | grep 2380
tcp   LISTEN 0      128    192.168.121.180:2380      0.0.0.0:*    uid:994 ino:27016 sk:f <->

$ etcdctl member list
8e9e05c52164694d, started, f724ca25024746ae87d4d7ece05438c5, http://localhost:2380, http://192.168.121.180:2379, false

$ etcdctl endpoint health
127.0.0.1:2379 is healthy: successfully committed proposal: took = 1.407527ms

$ etcdctl endpoint status
127.0.0.1:2379, 8e9e05c52164694d, 3.5.4, 29 kB, true, false, 4, 9, 9,
```

Finally, don't forget to configure the firewall if needed:

```bash
$ sudo systemctl enable firewalld
$ sudo systemctl start firewalld
$ sudo firewall-cmd --quiet --zone=public --add-port=2379/tcp --permanent
$ sudo firewall-cmd --quiet --zone=public --add-port=2380/tcp --permanent
$ sudo firewall-cmd --quiet --reload
```

### Watchdog

To avoid split-brain situations, Patroni supports [watchdog](https://patroni.readthedocs.io/en/latest/watchdog.html) devices.

> Watchdog devices are software or hardware mechanisms that will reset the whole system when they do not get a keepalive heartbeat within a specified timeframe. This adds an additional layer of fail safe in case usual Patroni split-brain protection mechanisms fail.

!!! Note
  While the use of a watchdog mechanism with Patroni is optional, it is recommended to use it for production environments.

Patroni will be the component interacting with the watchdog device. Set the permissions of the software watchdog on both **pg-patroni1** and **pg-patroni2** hosts:

```bash
$ cat <<EOF | sudo tee /etc/udev/rules.d/99-watchdog.rules
SUBSYSTEM=="misc", KERNEL=="watchdog", ACTION=="add", RUN+="/bin/setfacl -m u:postgres:rw- /dev/watchdog"
EOF
$ sudo sh -c 'echo "softdog" >> /etc/modules-load.d/softdog.conf'
$ sudo modprobe softdog
```

### PostgreSQL

On both **pg-patroni1** and **pg-patroni2** hosts, install PostgreSQL:

```bash
$ sudo dnf install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-8-x86_64/pgdg-redhat-repo-latest.noarch.rpm
$ sudo dnf -qy module disable postgresql
$ sudo dnf install -y postgresql14-server postgresql14-contrib
$ sudo systemctl disable postgresql-14
```

Patroni will bootstrap (create) the initial PostgreSQL cluster and be in charge of starting the service.

Don't forget to configure the firewall if needed:

```bash
$ sudo systemctl enable firewalld
$ sudo systemctl start firewalld
$ sudo firewall-cmd --quiet --permanent --add-service=postgresql
$ sudo firewall-cmd --quiet --reload
```

### Patroni

On both **pg-patroni1** and **pg-patroni2** hosts, install Patroni and its dependencies for etcd:

```bash
$ sudo dnf update -y libmodulemd
$ sudo dnf install -y epel-release
$ sudo dnf install -y patroni patroni-etcd
```

Let us now define the Patroni configuration in `/etc/patroni.yml`:

```bash
$ CLUSTER_NAME="demo-cluster-1"
$ MY_NAME=$(hostname --short)
$ MY_IP=$(hostname -I | awk ' {print $1}')
$ cat <<EOF | sudo tee /etc/patroni.yml
scope: $CLUSTER_NAME
namespace: /db/
name: $MY_NAME

restapi:
  listen: "0.0.0.0:8008"
  connect_address: "$MY_IP:8008"
  authentication:
    username: patroni
    password: mySupeSecretPassword

etcd:
    hosts:
    - etcd1:2379

bootstrap:
  dcs:
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    maximum_lag_on_failover: 1048576
    postgresql:
      use_pg_rewind: true
      use_slots: true
      parameters:
        archive_mode: "on"
        archive_command: "/bin/true"

  initdb:
  - encoding: UTF8
  - data-checksums

  pg_hba:
  - host replication replicator 0.0.0.0/0 scram-sha-256
  - host all all 0.0.0.0/0 scram-sha-256

  # Some additional users users which needs to be created after initializing new cluster
  users:
    admin:
      password: admin%
      options:
        - createrole
        - createdb

postgresql:
  listen: "0.0.0.0:5432"
  connect_address: "$MY_IP:5432"
  data_dir: /var/lib/pgsql/14/data
  bin_dir: /usr/pgsql-14/bin
  pgpass: /tmp/pgpass0
  authentication:
    replication:
      username: replicator
      password: confidential
    superuser:
      username: postgres
      password: my-super-password
    rewind:
      username: rewind_user
      password: rewind_password
  parameters:
    unix_socket_directories: '/var/run/postgresql,/tmp'

watchdog:
  mode: required
  device: /dev/watchdog
  safety_margin: 5

tags:
  nofailover: false
  noloadbalance: false
  clonefrom: false
  nosync: false
EOF
```

Finally, create the [systemd file](https://github.com/zalando/patroni/blob/master/extras/startup-scripts/patroni.service) and start the Patroni service:

```bash
$ cat <<EOF | sudo tee /lib/systemd/system/patroni.service
[Unit]
Description=Runners to orchestrate a high-availability PostgreSQL
After=syslog.target network.target

[Service]
Type=simple
User=postgres
Group=postgres
EnvironmentFile=-/etc/patroni_env.conf
ExecStart=/bin/patroni /etc/patroni.yml
ExecReload=/bin/kill -s HUP \$MAINPID
KillMode=process
TimeoutSec=30
Restart=no

[Install]
WantedBy=multi-user.target
EOF

$ sudo systemctl daemon-reload
$ sudo systemctl enable patroni
$ sudo systemctl start patroni
```

First, start the Patroni service on **pg-patroni1** so it becomes the leader of our PostgreSQL cluster, and then start **pg-patroni2**.

To list the members of the cluster, use the `patronictl` command:

```bash
$ patronictl -c /etc/patroni.yml list
+ Cluster: demo-cluster-1 (7104275668989116902) ----+----+-----------+
| Member      | Host            | Role    | State   | TL | Lag in MB |
+-------------+-----------------+---------+---------+----+-----------+
| pg-patroni1 | 192.168.121.61  | Leader  | running |  1 |           |
| pg-patroni2 | 192.168.121.135 | Replica | running |  1 |         0 |
+-------------+-----------------+---------+---------+----+-----------+
```

The Patroni REST API is available on port `8008`. Don't forget to configure the firewall if needed:

```bash
$ sudo firewall-cmd --quiet --zone=public --add-port=8008/tcp --permanent
$ sudo firewall-cmd --quiet --reload
```

### Connection strings

PostgreSQL [client libraries](https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING) like `libpq` and `jdbc` support client connection fail-over. The connection string contains multiple servers (eg: `host=srv1,srv2`) and the client library loops over the available hosts to find a connection that is available and capable of read-write or read-only operations. This capability allows clients to follow the primary cluster during a switchover.

Example:

```bash
$ psql "host=pg-patroni1,pg-patroni2 dbname=postgres user=admin target_session_attrs=read-write" -c "SELECT pg_is_in_recovery();"
 pg_is_in_recovery
-------------------
 f
(1 row)

$ psql "host=pg-patroni1,pg-patroni2 dbname=postgres user=admin target_session_attrs=read-only" -c "SELECT pg_is_in_recovery();"
 pg_is_in_recovery
-------------------
 t
(1 row)
```

### HAProxy

Instead of connecting directly to the database server, it is possible to setup HAProxy so the application will be connecting to the proxy instead, which will then forward the request to PostgreSQL. When HAproxy is used for this, it is also possible to route read-only requests to one or more replicas, for load balancing.

HAproxy can be installed as an independent server but it can also be installed on the application server or the database server itself.

For the purpose of this demo, let us install HAProxy on both **pg-patroni1** and **pg-patroni2** hosts:

```bash
$ sudo dnf install -y haproxy
$ sudo cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.bck
$ cat <<EOF | sudo tee /etc/haproxy/haproxy.cfg
global
    maxconn 100

defaults
    log    global
    mode    tcp
    retries 2
    timeout client 30m
    timeout connect 4s
    timeout server 30m
    timeout check 5s

listen stats
    mode http
    bind *:7000
    stats enable
    stats uri /

listen read-write
    bind *:5000
    option httpchk GET /read-write
    http-check expect status 200
    default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions
    server pg-patroni1 pg-patroni1:5432 maxconn 100 check port 8008
    server pg-patroni2 pg-patroni2:5432 maxconn 100 check port 8008

listen read-only
    balance roundrobin
    bind *:5001
    option httpchk OPTIONS /replica
    http-check expect status 200
    default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions
    server pg-patroni1 pg-patroni1:5432 maxconn 100 check port 8008
    server pg-patroni2 pg-patroni2:5432 maxconn 100 check port 8008
EOF

$ sudo setsebool -P haproxy_connect_any=1
$ sudo systemctl enable haproxy
$ sudo systemctl start haproxy
```

There are two sections: **read-write**, using port `5000`, and **read-only**, using port `5001`. All PostgreSQL hosts are included in both sections because they are both potential candidates to be either primary or standby. For HAproxy to know which role each host currently has, it will query the Patroni REST API:

```bash
$ curl -s http://pg-patroni1:8008
{"state": "running", "postmaster_start_time": "2022-06-01 14:17:57.445349+00:00", "role": "master", "server_version": 140003, "xlog": {"location": 50334688}, "timeline": 1, "replication": [{"usename": "replicator", "application_name": "pg-patroni2", "client_addr": "192.168.121.135", "state": "streaming", "sync_state": "async", "sync_priority": 0}], "dcs_last_seen": 1654095818, "database_system_identifier": "7104275668989116902", "patroni": {"version": "2.1.3", "scope": "demo-cluster-1"}}
```

!!! Note
  The `/replica` option will only redirect to standby server. Use `/read-only` to also include the primary.

Don't forget to configure the firewall if needed:

```bash
$ sudo firewall-cmd --quiet --zone=public --add-port=7000/tcp --permanent
$ sudo firewall-cmd --quiet --zone=public --add-port=5000/tcp --permanent
$ sudo firewall-cmd --quiet --zone=public --add-port=5001/tcp --permanent
$ sudo firewall-cmd --quiet --reload
```

Both `5000` and `5001` ports can now be used to chose between read-write or read-only connection:

```bash
$ psql -U admin -d postgres -h pg-patroni1 -p 5000 -c "SELECT pg_is_in_recovery();"
 pg_is_in_recovery
-------------------
 f
(1 row)

$ psql -U admin -d postgres -h pg-patroni1 -p 5001 -c "SELECT pg_is_in_recovery();"
 pg_is_in_recovery
-------------------
 t
(1 row)
```

### Keepalived

Finally, the question is: how to decide to which proxy host to connect to? The easiest solution is to use Keepalived and a VIP (virtual IP).

Let us then install it on each hosts running HAProxy (**pg-patroni1** and **pg-patroni2**):

```bash
$ sudo dnf install -y keepalived
$ sudo cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bck
```

For the purpose of this demo, we will use `192.168.121.121` as virtual IP. Configure then Keepalived:

```bash
$ VIP=192.168.121.121
$ MY_IP=$(hostname -I | awk ' {print $1}')
$ DEVICE=$(ip -br address | grep $MY_IP | awk '{print $1}')
$ cat <<EOF | sudo tee /etc/keepalived/keepalived.conf
global_defs {
  vrrp_garp_master_delay 3
  vrrp_garp_master_repeat 4
  vrrp_garp_master_refresh 60
  vrrp_garp_master_refresh_repeat 4
}
vrrp_instance patroni {
  state BACKUP
  interface $DEVICE
  virtual_router_id 100
  priority 100
  advert_int 1
  virtual_ipaddress {
    $VIP
  }
  authentication {
    auth_type PASS
    auth_pass secret
  }
}
EOF
```

Don't forget to configure the firewall if needed:

```bash
$ sudo firewall-cmd --add-rich-rule='rule protocol value="vrrp" accept' --permanent
$ sudo firewall-cmd --reload
```

Finally, enable and start the Keepalived service:

```bash
$ sudo systemctl enable keepalived
$ sudo systemctl start keepalived
```

Once started, the virtual IP should be attached to one of the hosts. Try to reach it:

```bash
$ ping -c 3 $VIP
PING 192.168.121.121 (192.168.121.121) 56(84) bytes of data.
64 bytes from 192.168.121.121: icmp_seq=1 ttl=64 time=0.196 ms
64 bytes from 192.168.121.121: icmp_seq=2 ttl=64 time=0.410 ms
64 bytes from 192.168.121.121: icmp_seq=3 ttl=64 time=0.322 ms

--- 192.168.121.121 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2052ms
rtt min/avg/max/mdev = 0.196/0.309/0.410/0.087 ms
```

It should now be possible to connect to the databases using the virtual IP too:

```bash
$ psql -U admin -d postgres -h $VIP -p 5000 -c "SELECT pg_is_in_recovery();"
 pg_is_in_recovery
-------------------
 f
(1 row)

$ psql -U admin -d postgres -h $VIP -p 5001 -c "SELECT pg_is_in_recovery();"
 pg_is_in_recovery
-------------------
 t
(1 row)
```
